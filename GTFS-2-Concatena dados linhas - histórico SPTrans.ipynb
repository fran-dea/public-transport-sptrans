{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433bcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Caminho onde estão os arquivos .zip\n",
    "caminho = 'dados/GTFS'\n",
    "\n",
    "# Lista dos arquivos GTFS\n",
    "arquivos_zip = [f for f in os.listdir(caminho) if f.endswith('.zip')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90b5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de arquivos GTFS que queremos processar\n",
    "gtfs_files = [\n",
    "    \"agency.txt\", \"calendar.txt\", \"fare_attributes.txt\", \"fare_rules.txt\",\n",
    "    \"frequencies.txt\", \"routes.txt\", \"shapes.txt\", \"stop_times.txt\",\n",
    "    \"stops.txt\", \"trips.txt\"\n",
    "]\n",
    "\n",
    "# Dicionário para acumular DataFrames por tabela\n",
    "dfs_acumulados = {arquivo.replace(\".txt\", \"\"): [] for arquivo in gtfs_files}\n",
    "\n",
    "# Iterar sobre cada arquivo .zip\n",
    "for arquivo in arquivos_zip:\n",
    "    data_ref = arquivo.replace(\"google_transit_\", \"\").replace(\".zip\", \"\")\n",
    "    caminho_arquivo = os.path.join(caminho, arquivo)\n",
    "\n",
    "    with zipfile.ZipFile(caminho_arquivo, \"r\") as z:\n",
    "        for gtfs_file in gtfs_files:\n",
    "            if gtfs_file in z.namelist():\n",
    "                df = pd.read_csv(z.open(gtfs_file))\n",
    "                df[\"data_referencia\"] = data_ref\n",
    "                dfs_acumulados[gtfs_file.replace(\".txt\", \"\")].append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames e salvar em parquet\n",
    "os.makedirs(\"dados\", exist_ok=True)\n",
    "for nome, lista_dfs in dfs_acumulados.items():\n",
    "    if lista_dfs:  # só concatena se houver dados\n",
    "        final_df = pd.concat(lista_dfs, ignore_index=True)\n",
    "        final_df.to_parquet(f\"dados/{nome}_historico.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "043a06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataframes já concatenados\n",
    "trips_df = pd.read_parquet(\"dados/trips_historico.parquet\")\n",
    "routes_df = pd.read_parquet(\"dados/routes_historico.parquet\")\n",
    "stop_times_df = pd.read_parquet(\"dados/stop_times_historico.parquet\")\n",
    "stops_df = pd.read_parquet(\"dados/stops_historico.parquet\")\n",
    "\n",
    "# Limpeza e ajustes\n",
    "stop_times_df = stop_times_df[['trip_id', 'stop_id', 'stop_sequence', 'data_referencia']].drop_duplicates()\n",
    "trips_df['route_id'] = trips_df['route_id'].str.replace(\"-\", \"\")\n",
    "routes_df['route_id'] = routes_df['route_id'].str.replace(\"-\", \"\")\n",
    "\n",
    "services = {\n",
    "    'USD': \"Todos\", 'U__': \"Úteis\", 'US_': \"Úteis e Sábados\",\n",
    "    '_SD': \"Sábados e Domingos\", '__D': \"Domingos\", '_S_': \"Sábados\"\n",
    "}\n",
    "trips_df['service_id'] = trips_df['service_id'].map(services)\n",
    "\n",
    "# Merge final\n",
    "trips_df = trips_df.merge(stop_times_df, on=['trip_id', 'data_referencia']).merge(stops_df, on=['stop_id', 'data_referencia'])\n",
    "\n",
    "# Salvar versões tratadas\n",
    "routes_df.to_parquet('dados/routes_historico.parquet', index=False)\n",
    "trips_df.to_parquet('dados/trips_historico.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
